#' @title Function to evaluate relative importance of each variable.
#' @description Evaluate relative importance of each variable within the model
#' using the following methods:
#' \itemize{
#' \item{Jackknife test based on AUC ratio and Pearson correlation between the
#' result of model using all variables}
#' \item{Shapley Additive Explanations (SHAP) according to Shapley values}}
#' @param model (`isolation_forest`) The extended isolation forest SDM. It could be
#' the item `model` of `POIsotree` made by function \code{\link{isotree_po}}.
#' @param var_occ (`data.frame`, `tibble`) The `data.frame` style table that
#' include values of environmental variables at occurrence locations.
#' @param var_occ_test (`data.frame`, `tibble`, or `NULL`) The `data.frame` style
#' table that include values of environmental variables at occurrence locations of test.
#' If `NULL`, it would be set the same as `var_occ`. The default is `NULL`.
#' @param variables (`stars`) The `stars` of environmental variables. It should have
#' multiple `attributes` instead of `dims`. If you have `raster` object instead, you
#' could use \code{\link{st_as_stars}} to convert it to `stars` or use
#' \code{\link{read_stars}} directly read source data as a `stars`.
#' @param shap_nsim (`integer`) The number of Monte Carlo repetitions in SHAP
#' method to use for estimating each Shapley value. See details in documentation of
#' function \code{\link{explain}} in package `fastshap`.
#' @param visualize (`logical`) If `TRUE`, plot the analysis figures.
#' The default is `FALSE`.
#' @param seed (`integer`) The seed for any random progress. The default is `10L`.
#' @return (`VariableAnalysis`) A list of
#' \itemize{
#' \item{variables (`vector` of `character`) The names of environmental variables}
#' \item{pearson_correlation (`tibble`) A table of Jackknife test based on Pearson correlation}
#' \item{full_AUC_ratio (`tibble`) A table of AUC ratio of training and test dataset using all variables,
#' that act as references for Jackknife test}
#' \item{AUC_ratio (`tibble`) A table of Jackknife test based on AUC ratio}
#' \item{SHAP (`tibble`) A table of Shapley values of training and test dataset separately}
#' }
#'
#' @seealso
#' \code{\link{plot.VariableAnalysis}}, \code{\link{print.VariableAnalysis}}
#' \code{\link{explain}} in `fastshap`
#'
#' @details
#' \bold{Jackknife test} of variable importance is reflected as the decrease
#' in a model performance when an environmental variable is used singly or is
#' excluded from the environmental variable pool. In this function, we used
#' Pearson correlation and AUC ratio.
#'
#' \bold{Pearson correlation} is the correlation between the predictions generated by
#' different variable importance evaluation methods and the predictions generated
#' by the full model as the assessment of mode performance.
#'
#' The area under the ROC curve (AUC) is a threshold-independent evaluator of
#' model performance, which needs both presence and absence data. A ROC curve is
#' generated by plotting the proportion of correctly predicted presence on the
#' y-axis against 1 minus the proportion of correctly predicted absence on x-axis
#' for all thresholds. Multiple approaches have been used to evaluate accuracy of
#' presence-only models. Peterson et al. (2008) modified AUC by plotting the
#' proportion of correctly predicted presence against the proportion of
#' presences falling above a range of thresholds against the proportion of
#' cells of the whole area falling above the range of thresholds. This is the
#' so called \bold{AUC ratio} that is used in this package.
#'
#' \bold{Shapley Additive Explanations (SHAP)} uses Shapley values to evaluate the variable importance. The
#' larger the absolute value of Shapley value, the more important this variable is.
#' Positive Shapley values mean positive affect, while negative Shapely values mean
#' negative affect. Please check references for more details if you are interested in.
#'
#' @references
#' \itemize{
#' \item{\href{https://doi.org/10.1016/j.ecolmodel.2007.11.008}{Peterson,
#' A. Townsend, Monica Papeş, and Jorge Soberón. "Rethinking receiver operating
#' characteristic analysis applications in ecological niche modeling."
#' \emph{Ecological modelling} 213.1 (2008): 63-72.}}
#' \item{\href{https://doi.org/10.1007/s10115-013-0679-x}{Strumbelj, Erik,
#' and Igor Kononenko. "Explaining prediction models and individual predictions
#' with feature contributions." \emph{Knowledge and information systems}
#' 41.3 (2014): 647-665.}}
#' \item{\href{http://proceedings.mlr.press/v119/sundararajan20b.html}{Sundara
#' rajan, Mukund, and Amir Najmi. "The many Shapley values for model explanation
#' ." \emph{International Conference on Machine Learning}. PMLR, 2020.}}
#' \item{\href{https://github.com/bgreenwell/fastshap}{https://github.com/
#' bgreenwell/fastshap}}
#' \item{\href{https://github.com/slundberg/shap}{https://github.com/slundberg/shap}}
#' }
#'
#' @importFrom dplyr select tibble filter sample_n
#' @importFrom sf st_as_sf st_drop_geometry
#' @importFrom stars st_as_stars st_xy2sfc st_get_dimension_values
#' @importFrom fastshap explain
#' @importFrom stats cor predict
#' @importFrom tidyselect all_of
#' @export
#' @examples
#' # Using a pseudo presence-only occurrence dataset of
#' # virtual species provided in this package
#' library(dplyr)
#' library(sf)
#' library(stars)
#' library(itsdm)
#'
#' data("occ_virtual_species")
#' occ_virtual_species <- occ_virtual_species %>%
#'   mutate(id = row_number())
#'
#' set.seed(11)
#' occ <- occ_virtual_species %>% sample_frac(0.7)
#' occ_test <- occ_virtual_species %>% filter(! id %in% occ$id)
#' occ <- occ %>% select(-id)
#' occ_test <- occ_test %>% select(-id)
#'
#' env_vars <- system.file(
#'   'extdata/bioclim_africa_10min.tif',
#'   package = 'itsdm') %>% read_stars() %>%
#'   slice('band', c(1, 12))
#'
#' mod <- isotree_po(
#'   occ = occ, occ_test = occ_test,
#'   variables = env_vars, ntrees = 200,
#'   sample_rate = 0.8, ndim = 0L,
#'   seed = 123L, response = FALSE,
#'   check_variable = FALSE)
#'
#' var_analysis <- variable_analysis(
#'   model = mod$model,
#'   var_occ = mod$var_train %>% st_drop_geometry(),
#'   var_occ_test = mod$var_test %>% st_drop_geometry(),
#'   variables = mod$variables)
#'
variable_analysis <- function(model,
                              var_occ,
                              var_occ_test = NULL, # Independent test
                              variables,
                              shap_nsim = 100,
                              visualize = FALSE,
                              seed = 10) {
  # Check inputs
  checkmate::assert_data_frame(var_occ)
  checkmate::assert_data_frame(var_occ_test, null.ok = T)
  if (is.null(var_occ_test)) {
    warning(paste0('var_occ_test is NULL, set it to var_occ. ',
                   'The result of train and test would be the same'))
    var_occ_test <- var_occ
  }
  checkmate::assert_class(variables, 'stars')
  checkmate::assert_int(shap_nsim)
  checkmate::assert_logical(visualize)
  checkmate::assert_int(seed)
  bands <- names(variables)
  stopifnot(all(bands %in% colnames(var_occ)))

  # Sampling from the whole image to speed things up.
  set.seed(seed)
  samples <- variables %>% select(bands[1]) %>%
    st_xy2sfc(as_points = T) %>% st_as_sf() %>%
    select(geometry) %>%
    sample_n(min(10000, nrow(.)))
  vars <- st_extract(x = variables, at = samples) %>%
    st_drop_geometry()
  rm(samples, variables)

  # Original prediction
  ## Predict
  full_pred_occ <- 1 - predict(model, var_occ)
  full_pred_occ_test <- 1 - predict(model, var_occ_test)
  full_pred_var <- 1 - predict(model, vars)

  ## Stretch to 0 to 1.
  full_pred_occ <- .stretch(x = full_pred_var,
                            new_values = full_pred_occ,
                            minq = 0)
  full_pred_occ_test <- .stretch(x = full_pred_var,
                                 new_values = full_pred_occ_test,
                                 minq = 0)
  full_pred_var <- .stretch(x = full_pred_var,
                            minq = 0)

  ## AUC background and ratio
  full_auc_train <- .auc_ratio(full_pred_occ, full_pred_var)
  full_auc_test <- .auc_ratio(full_pred_occ_test, full_pred_var)

  # Process
  var_each <- do.call(rbind, lapply(bands, function(nm){
    # Model with only this variable
    ## Subset dataset
    this_var_occ <- var_occ %>% select(all_of(nm))
    this_var_occ_test <- var_occ_test %>% select(all_of(nm))
    this_vars <- vars %>% select(all_of(nm))

    ## Fit model
    this_model <- isolation.forest(
      this_var_occ,
      ntrees = model$params$ntrees,
      sample_size = model$params$sample_size,
      ndim = 1,
      ntry = model$params$ntry,
      max_depth = model$params$max_depth,
      prob_pick_avg_gain = model$params$prob_pick_avg_gain,
      prob_pick_pooled_gain = model$params$prob_pick_pooled_gain,
      prob_split_avg_gain = model$params$prob_split_avg_gain,
      prob_split_pooled_gain = model$params$prob_split_pooled_gain,
      min_gain = model$params$min_gain,
      missing_action = model$params$missing_action,
      categ_split_type = model$params$categ_split_type,
      all_perm = model$params$all_perm,
      coef_by_prop = model$params$coef_by_prop,
      weights_as_sample_prob = model$params$weights_as_sample_prob,
      sample_with_replacement = model$params$sample_with_replacement,
      penalize_range = model$params$penalize_range,
      weigh_by_kurtosis = model$params$weigh_by_kurtosis,
      coefs = model$params$coefs,
      assume_full_distr = model$params$assume_full_distr,
      build_imputer = model$params$build_imputer,
      min_imp_obs = model$params$min_imp_obs,
      depth_imp = model$params$depth_imp,
      weigh_imp_rows = model$params$weigh_imp_rows)

    ## Prediction
    this_occ_pred <- 1 - predict(this_model, this_var_occ)
    this_occ_test_pred <- 1 - predict(this_model, this_var_occ_test)
    this_vars_pred <- 1 - predict(this_model, this_vars)

    # Stretch
    this_occ_pred <- .stretch(x = this_vars_pred,
                              new_values = this_occ_pred,
                              minq = 0)
    this_occ_test_pred <- .stretch(x = this_vars_pred,
                                   new_values = this_occ_test_pred,
                                   minq = 0)
    this_vars_pred <- .stretch(this_vars_pred,
                               minq = 0)

    ## Calculate metrics
    r_only_train <- cor(full_pred_occ, this_occ_pred,
                        use = 'complete.obs')
    r_only_test <- cor(full_pred_occ_test, this_occ_test_pred,
                       use = 'complete.obs')
    auc_only_train <- .auc_ratio(this_occ_pred, this_vars_pred)
    auc_only_test <- .auc_ratio(this_occ_test_pred, this_vars_pred)

    # Model with variables except the chosen one
    ## Subset dataset
    nms <- setdiff(bands, nm)
    except_var_occ <- var_occ %>% select(all_of(nms))
    except_var_occ_test <- var_occ_test %>% select(all_of(nms))
    except_vars <- vars %>% select(all_of(nms))

    ## Fit model
    except_model <- isolation.forest(
      except_var_occ,
      ntrees = model$params$ntrees,
      sample_size = model$params$sample_size,
      ndim = ifelse(ncol(except_var_occ) >= model$params$ndim,
                    model$params$ndim, ncol(except_var_occ)),
      ntry = model$params$ntry,
      max_depth = model$params$max_depth,
      prob_pick_avg_gain = model$params$prob_pick_avg_gain,
      prob_pick_pooled_gain = model$params$prob_pick_pooled_gain,
      prob_split_avg_gain = model$params$prob_split_avg_gain,
      prob_split_pooled_gain = model$params$prob_split_pooled_gain,
      min_gain = model$params$min_gain,
      missing_action = model$params$missing_action,
      categ_split_type = model$params$categ_split_type,
      all_perm = model$params$all_perm,
      coef_by_prop = model$params$coef_by_prop,
      weights_as_sample_prob = model$params$weights_as_sample_prob,
      sample_with_replacement = model$params$sample_with_replacement,
      penalize_range = model$params$penalize_range,
      weigh_by_kurtosis = model$params$weigh_by_kurtosis,
      coefs = model$params$coefs,
      assume_full_distr = model$params$assume_full_distr,
      build_imputer = model$params$build_imputer,
      min_imp_obs = model$params$min_imp_obs,
      depth_imp = model$params$depth_imp,
      weigh_imp_rows = model$params$weigh_imp_rows)

    ## Prediction
    except_occ_pred <- 1 - predict(except_model, except_var_occ)
    except_occ_test_pred <- 1 - predict(except_model, except_var_occ_test)
    except_vars_pred <- 1 - predict(except_model, except_vars)

    ## Stretch
    except_occ_pred <- .stretch(x = except_vars_pred,
                                new_values = except_occ_pred,
                                minq = 0)
    except_occ_test_pred <- .stretch(x = except_vars_pred,
                                     new_values = except_occ_test_pred,
                                     minq = 0)
    except_vars_pred <- .stretch(except_vars_pred,
                                 minq = 0)

    ## Calculate metrics
    r_except_train <- cor(full_pred_occ, except_occ_pred,
                          use = 'complete.obs')
    r_except_test <- cor(full_pred_occ_test, except_occ_test_pred,
                          use = 'complete.obs')
    auc_except_train <- .auc_ratio(except_occ_pred, except_vars_pred)
    auc_except_test <- .auc_ratio(except_occ_test_pred, except_vars_pred)

    # Clean up
    rm(this_var_occ, this_var_occ_test, this_vars, this_model, this_occ_pred,
       this_occ_test_pred, nms, except_var_occ, except_var_occ_test, except_vars,
       except_model, except_occ_pred, except_occ_test_pred)

    # Output
    tibble(variable = rep(nm, 8),
           metrics = c(rep('pearson_correlation', 4), rep('AUC_ratio', 4)),
           method = rep(c(rep('Only', 2), rep('Without', 2)), 2),
           usage = rep(c('Train', 'Test'), 4),
           value = c(r_only_train, r_only_test,
                     r_except_train, r_except_test,
                     auc_only_train, auc_only_test,
                     auc_except_train, auc_except_test))
  }))

  # NOTE: Permutation feature importance method does not work for isolation forest
  # because each split is independent to the rank of values. Its tree structure is
  # fundamentally different from tree structure such as random forest.
  # ALTERNATIVE: SHAP method that use Shapley values according to game theory.

  # SHAP feature importance
  set.seed(seed)
  shap_train <- explain(model, X = var_occ, nsim = shap_nsim,
                        pred_wrapper = .pfun_shap)
  set.seed(seed)
  shap_test <- explain(model, X = var_occ, nsim = shap_nsim,
                       newdata = var_occ_test,
                       pred_wrapper = .pfun_shap)

  # Output
  out <- list(variables = bands,
              pearson_correlation =
                var_each %>%
                filter(metrics == 'pearson_correlation') %>%
                select(-metrics),
              full_AUC_ratio =
                tibble(full_auc_train = full_auc_train,
                       full_auc_test = full_auc_test),
              AUC_ratio =
                var_each %>%
                filter(metrics == 'AUC_ratio') %>%
                select(-metrics),
              SHAP = list(train = shap_train,
                          test = shap_test))

  class(out) <- append("VariableAnalysis", class(out))

  # Visualize
  if (visualize) {
    plot(out)
  }

  # Return
  return(out)
}

# variable_analysis end
